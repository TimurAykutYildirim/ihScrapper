# -*- coding: utf-8 -*-

import urllib2
import json
from Tkinter import *

url = 'http://api.usatoday.com/open/articles/topnews?encoding=json&api_key=nm4dsscmcw7uzgsmbzshzztm'
url2 = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=pharma&page=pageNum&api-key=f8f3f3386b84e418b8cec379f732e0c0%3A1%3A70152454'
url3 = 'http://78.46.106.130:83/demo_apa_ots'
url4 = 'http://content.guardianapis.com/?api-key=8zxduw5zum7m58ub5rwehytc&show-most-viewed=true&q=pfizer'


# this will contain resulting query string that shapes according to chosen api and menu item
queryString = ''

def but1():
    print (var.get())
    if(str(var.get()) == 'NY Times Article Search API'):
        b1.configure(command=ny_times_article_search_scrapper)
    elif(str(var.get()) == 'USA Today Articles API'):
        b1.configure(command=usa_today_articles_scrapper)
    elif(str(var.get()) == 'APA OTS News API'):
        b1.configure(command=apa_ots_news_scrapper)
    elif(str(var.get()) == 'Guardian Content API'):
        b1.configure(command=guardian_content_scrapper)
    elif(str(var.get()) == 'Choose News Source'):
        tf.insert(INSERT, "You should choose something")
    else:
        tf.insert(INSERT, "Invalid choice")

def usa_today_articles_scrapper():
    try:
        response = urllib2.urlopen(url)

        data = json.loads(response.read())

        tf.delete(1.0, END)

        text_file = open("usa_today_links.txt", "w")
        for index in range(len(data['stories'])):
            if index < len(data['stories']) - 1:
                text_file.writelines(data['stories'][index]['link'] + "\n")
                print (data['stories'][index]['link'])
                tf.insert(INSERT, data['stories'][index]['link'] + "\n")
            else:
                text_file.writelines(data['stories'][index]['link'])
                print (data['stories'][index]['link'])
                tf.insert(INSERT, data['stories'][index]['link'])
                tf.config(state=DISABLED)

        text_file.close()

    except urllib2.URLError, e:
        print ("something went wrong" + e)

def ny_times_looper():
    try:
        for counter in range(1,110):
            newUrl =  url2.replace("pageNum", str(counter))
            ny_times_article_search_scrapper(newUrl)
            #print newUrl

    except Exception as error:
        print error


# x burada ny_times_looper methodundan gelecek url'leri sırayla ny_times_article_search_scrapper methoduna gönderiyor

def ny_times_article_search_scrapper(x):
    try:
        response = urllib2.urlopen(x)

        data = json.loads(response.read())

        tf.delete(1.0, END)

        text_file = open("ny_times_links.txt", "a")
        for index in range(len(data['response']['docs'])):
            if index < len(data['response']['docs']) - 1:
                text_file.writelines(data['response']['docs'][index]['web_url'] + "\n")
                print (data['response']['docs'][index]['web_url'])
                tf.insert(INSERT, data['response']['docs'][index]['web_url'] + "\n")
            else:
                text_file.writelines(data['response']['docs'][index]['web_url'])
                print (data['response']['docs'][index]['web_url'])
                tf.insert(INSERT, data['response']['docs'][index]['web_url'])
                tf.config(state=DISABLED)

        text_file.close()

    except urllib2.URLError, e:
        print ("something went wrong" + e)
'''
def ny_times_article_search_scrapper():
    try:
        response = urllib2.urlopen(url2)

        data = json.loads(response.read())

        tf.delete(1.0, END)

        text_file = open("ny_times_links.txt", "a")
        for index in range(len(data['response']['docs'])):
            if index < len(data['response']['docs']) - 1:
                text_file.writelines(data['response']['docs'][index]['web_url'] + "\n")
                print (data['response']['docs'][index]['web_url'])
                tf.insert(INSERT, data['response']['docs'][index]['web_url'] + "\n")
            else:
                text_file.writelines(data['response']['docs'][index]['web_url'])
                print (data['response']['docs'][index]['web_url'])
                tf.insert(INSERT, data['response']['docs'][index]['web_url'])
                tf.config(state=DISABLED)

        text_file.close()

    except urllib2.URLError, e:
        print ("something went wrong" + e)
'''

def apa_ots_news_scrapper():
    try:
        response = urllib2.urlopen(url3)

        str(response).decode('string-escape')

        data = json.loads(response.read())

        tf.delete(1.0, END)

        text_file = open("apa_ots_links.txt", "w")
        for index in range(len(data['ergebnisse'])):
            if index < len(data['ergebnisse']) - 1:
                text_file.writelines(data['ergebnisse'][index]['WEBLINK'] + "\n")
                print (data['ergebnisse'][index]['WEBLINK'])
                tf.insert(INSERT, data['ergebnisse'][index]['WEBLINK'] + "\n")
            else:
                text_file.writelines(data['ergebnisse'][index]['WEBLINK'])
                print (data['ergebnisse'][index]['WEBLINK'])
                tf.insert(INSERT, data['ergebnisse'][index]['WEBLINK'])
                tf.config(state=DISABLED)

        text_file.close()

    except urllib2.URLError, e:
        print ("something went wrong" + e)

def guardian_content_scrapper():
    try:
        response = urllib2.urlopen(url4)

        data = json.loads(response.read())

        tf.delete(1.0, END)

        text_file = open("guardian_links.txt", "w")
        for index in range(len(data['response']['mostViewed'])):
            if index < len(data['response']['mostViewed']) - 1:
                text_file.writelines(data['response']['mostViewed'][index]['webUrl'] + "\n")
                print (data['response']['mostViewed'][index]['webUrl'])
                tf.insert(INSERT, data['response']['mostViewed'][index]['webUrl'] + "\n")
            else:
                text_file.writelines(data['response']['mostViewed'][index]['webUrl'])
                print (data['response']['mostViewed'][index]['webUrl'])
                tf.insert(INSERT, data['response']['mostViewed'][index]['webUrl'])
                tf.config(state=DISABLED)

        text_file.close()

    except urllib2.URLError, e:
        print ("something went wrong" + e)

#designing gui items
myGUI = Tk()
myGUI.title("ihScrapper")

#defining combobox which lists news sources
var = StringVar(myGUI)
var.set("Choose News Source")
c1 = OptionMenu(myGUI, var, "NY Times Article Search API", "USA Today Articles API", "APA OTS News API", "Guardian Content API")
c1.pack()

# defining buttons
b1 = Button(myGUI, text="Go", padx=50)
b2 = Button(myGUI, text="USA Today Articles API", padx=50)
b3 = Button(myGUI, text="NY Times Article Search API", padx=50)
b4 = Button(myGUI, text="APA OTS News API", padx=50)
b5 = Button(myGUI, text="Guardian Content API", padx=50)

# defining textfield
tf = Text(myGUI)


#positioning items on gui
c1.grid(row=0, column=0)
b1.grid(row=1, column=0)
b2.grid(row=2, column=0)
b3.grid(row=3, column=0)
b4.grid(row=4, column=0)
b5.grid(row=5, column=0)
tf.grid(row=6, column=0)

#assigning methods on button click
b1.configure(command=ny_times_looper)
b2.configure(command=usa_today_articles_scrapper)
b3.configure(command=ny_times_article_search_scrapper)
b4.configure(command=apa_ots_news_scrapper)
b5.configure(command=guardian_content_scrapper)
myGUI.mainloop()
